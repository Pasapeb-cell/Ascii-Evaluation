{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SketchKeras.sketch_model import SketchKeras\n",
    "import SketchKeras.sketchkeras as SK\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image, n, m):  \n",
    "    patches = []\n",
    "    image = np.array(image)\n",
    "    height, width = image.shape[:2]\n",
    "    images_height =0\n",
    "    images_width = 0\n",
    "\n",
    "    for y in range(0, height - n + 1, m):\n",
    "        images_height += 1\n",
    "        for x in range(0, width - n + 1, m):  \n",
    "            images_width += 1          \n",
    "            patch = image[y:y+n, x:x+n]\n",
    "            patches.append(patch)\n",
    "    images_width = images_width//images_height\n",
    "\n",
    "    return np.array([patch for patch in patches]), images_width, images_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = list(glob.glob('./*.jpg'))\n",
    "print(\"Images:\", len(image_names))\n",
    "encoded_image = model.encode([Image.open(filepath) for filepath in image_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Now we run the clustering algorithm. This function compares images aganist \n",
    "# all other images and returns a list with the pairs that have the highest \n",
    "# cosine similarity score\n",
    "processed_images = util.paraphrase_mining_embeddings(encoded_image)\n",
    "NUM_SIMILAR_IMAGES = 10 \n",
    "\n",
    "# =================\n",
    "# DUPLICATES\n",
    "# =================\n",
    "print('Finding duplicate images...')\n",
    "# Filter list for duplicates. Results are triplets (score, image_id1, image_id2) and is scorted in decreasing order\n",
    "# A duplicate image will have a score of 1.00\n",
    "# It may be 0.9999 due to lossy image compression (.jpg)\n",
    "duplicates = [image for image in processed_images if image[0] >= 0.999]\n",
    "\n",
    "# Output the top X duplicate images\n",
    "for score, image_id1, image_id2 in duplicates[0:NUM_SIMILAR_IMAGES]:\n",
    "    print(\"\\nScore: {:.3f}%\".format(score * 100))\n",
    "    print(image_names[image_id1])\n",
    "    print(image_names[image_id2])\n",
    "\n",
    "# =================\n",
    "# NEAR DUPLICATES\n",
    "# =================\n",
    "print('Finding near duplicate images...')\n",
    "# Use a threshold parameter to identify two images as similar. By setting the threshold lower, \n",
    "# you will get larger clusters which have less similar images in it. Threshold 0 - 1.00\n",
    "# A threshold of 1.00 means the two images are exactly the same. Since we are finding near \n",
    "# duplicate images, we can set it at 0.99 or any number 0 < X < 1.00.\n",
    "threshold = 0.99\n",
    "near_duplicates = [image for image in processed_images if image[0] < threshold]\n",
    "\n",
    "for score, image_id1, image_id2 in near_duplicates[0:NUM_SIMILAR_IMAGES]:\n",
    "    print(\"\\nScore: {:.3f}%\".format(score * 100))\n",
    "    print(image_names[image_id1])\n",
    "    print(image_names[image_id2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
